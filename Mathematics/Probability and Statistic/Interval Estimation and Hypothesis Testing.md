# 说明
统计里面的区间估计和假设检验有密切联系。这种关系就好比，正向推导与反证法的关系。下面给出一些参量和术语的对应关系。


|区间估计的统计量|假设检验的统计量|
|-|-|
|枢轴量<br>Pivotal Value|检验统计量<br>Test Statistics|
|置信水平<br> $1-\alpha$ |检验水平<br> $\alpha$ |
|临界值|临界值|
|置信区间|否定域|

这二者本质上解决的问题相同，描述的现象相同，但是处理问题的流程和思路又有区别，需要注意。

# 假设检验中的检验统计量
补充一下，关于等号的检验，都可以修正转化为对不等号的检验，相当于和临界值比较之前，进行了不等式放缩。

这一点要在考试前重点复习一下。

另外，两个正态总体的比较是常见的问题。**方差的比较一般做比，而期望的比较一般做差**。这么做的道理在于，正态分布的线性组合（这里是做差）还是正态分布。而 $\chi^2$ 分布对于求和可以保持，对于做差就不可以了。恰好做比，可以构造另一种已知的分布，也就是 $F$ 分布。

## 服从 $N(0,1)$ 分布的检验统计量
很多问题，只要样本量足够大，都可以近似用标准正态分布检验。

构造这类检验统计量的**出发点是已知一个满足正态分布的统计量，思路是对它标准化**。
### 一个正态总体的期望值检验（已知方差）
已知 $\sigma^2,H_0:\mu=\mu_0$

$$
N=\frac{\bar X-\mu}{\sqrt{\frac{\sigma^2}{n}}}=\frac{\bar X-\mu_0}{\sqrt{\frac{\sigma^2}{n}}}\sim N(0,1)
$$

### 两个 Bernoulli 总体的成功率（比率）检验
$H_0: p_1=p_2$

$$
\hat p_i = \frac{1}{n_1}\sum_{i}^{n_1} X_i=\frac{S_i}{n_1}
$$

$$
\xi = \frac{\hat p_1-\hat p_2-(p_1-p_2)}{\sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2}}}=\frac{\hat p_1-\hat p_2}{\sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2}}}\sim N(0,1)
$$

## 服从 $\chi^2$ 分布的检验统计量
$\chi^2$ 分布的定义是若干标准正态分布的平方和。

从这样的定义可以看出，涉及平方和的假设检验往往离不开 $\chi^2$ 分布。

什么地方会涉及平方和？除了方差之外？

平方和描述的是一种二范数下定义的广义“距离”。那么，凡是涉及到“距离”或者和某一个固定参考点的“差异”比较问题，就往往会涉及到平方和。方差本身，也是描述了样本到其均值的“距离”。

具体构造检验统计量，不仅仅是求平方和。还要在平方求和以前，逐项标准化（各项服从正态分布）。

也就是说，构造这一类统计量的**出发点是“距离”和某一定值的比较**，构造**思路是逐项标准化，然后求平方和**。
### 分布函数检验（拟合优度检验）
已知分布是零假设，
可以有力地否定分布函数。


对 $\nu_i$ 进行标准化然后求平方和。

$$
V=\sum_{i=1}^{m+1}\frac{(\nu_i-np_i)^2}{np_i}\sim \chi^2(m)
$$

### 独立性检验
独立性是零假设，
可以有力地否定独立性。

$$
\sum_{i,j\in \{1,2\},i\neq j}\frac{(nP(A_iA_j)-nP(A_i)P(A_j))^2}{(nP(A_i)P(A_j))^2}\sim \chi^2(1)
$$

### 一个正态总体的方差检验

$H_0:\sigma^2=\sigma _0^2$

$$
W=\sum_{i=1}^n \frac{(X_i-\bar X)}{\sigma^2}
=\sum_{i=1}^n \frac{(X_i-\bar X)}{\sigma_0^2}\sim \chi^2(n-1)
$$

## 服从 $F$ 分布的检验统计量
$F$ 分布的定义和方差做比有密切关系。所以这一类检验统计量的构造的思路是，做比。并且，构造的前提，具有平方和的属性。

$F$ 分布的定义是，两个 $\chi^2$ 分布各自平均（除以自由度）之后的比值。

所以， $F$ 分布类型的检验统计量的构造出**发点是比较两个总体在某种意义下的“距离”，思路是首先各自构造 $\chi^2$ 类型统计量，然后各自平均（除以自由度），最后做比**。
### 线性回归问题中的相关性检验
线性无关是零假设，
可以有力地肯定线性相关性。

$$
F=\frac{U}{\frac{Q}{n-2}}\sim F(1,n-2)
$$

这里的 $U,Q$ 分别为回归平方和与残差平方和。

$$
U=\sum_t(\bar y-\hat y_t)^2=\hat b^2 l_{XX}
$$

$$
Q=\sum_t(y_t-\hat y_t)^2=l_{YY}-U
$$

这种检验等价于用相关系数 $R$ 进行检验。

### 两个正态总体的方差检验
####  $H_0:\sigma_1^2=\sigma_2^2$

$$
F=\frac{\frac{S_1^2}{\sigma_1^2}}{\frac{S_2^2}{\sigma_2^2}}=\frac{S_1^2}{S_2^2}\sim F(n_1-1,n_2-1)
$$

### 一个 Bernoulli 总体成功率（比率）的假设检验
具体比较复杂，但是临界值的计算涉及到 $F$ 分布。


## 服从 $t$ 分布的检验统计量
这种做法和正态分布的检验统计量构造思路一致，都是标准化（减去期望值，除以标准差）。只不过，标准差是通过样本估计出来的，而不是明确已知的。所以，主要用于对期望值的假设检验（利用样本均值）。

另外， $t$ 分布的定义本身就和标准化的过程密切相关，是一个标准正态分布，除以一个 $\chi^2$ 分布的均方根。自由度是继承的关系。

总结一下， $t$ 分布类型检验统计量构造的**出发点是某一个服从正态分布的统计量，并且对其的标准化遇到困难，因为方差未知。思路就是标准化，并且用样本方差估计未知的总体方差**。

另外， $t$ 分布的平方满足特殊的 $F$ 分布。
### 一个正态总体的期望值检验
未知 $\sigma^2,H_0:\mu=\mu_0$

$$
T=\frac{\bar X-\mu}{\sqrt{\frac{S^2}{n}}}=\frac{\bar X-\mu_0}{\sqrt{\frac{S^2}{n}}}\sim T(n-1)
$$

### 成对数据的期望值检验
$H_0:\mu_z=0$
做差化成正态单总体（差值服从正态分布，各自未必服从）

$$
Z=X-Y
$$

转化为正态单总体的情况。

### 两个正态总体的期望值检验


#### 已知 $\sigma_1^2=\sigma_2^2,\ H_0:\mu_1=\mu_2$ 
对 $\bar X-\bar Y$ 标准化：


$$
T=\frac{\bar X-\bar Y-(\mu_1-\mu_2)}{\sqrt{\frac{S_1^2}{n}+\frac{S_2^2}{n}}}=\frac{\bar X-\bar Y}{\sqrt{\frac{S_1^2}{n}+\frac{S_2^2}{n}}}\sim t(2n-2)
$$

#### 已知 $\sigma_1^2\neq\sigma_2^2,\ H_0:\mu_1=\mu_2$ 

对 $\bar X-\bar Y$ 标准化：


$$
T=\frac{\bar X-\bar Y-(\mu_1-\mu_2)}{\sqrt{\frac{S_1^2}{n}+\frac{S_2^2}{n}}}=\frac{\bar X-\bar Y}{\sqrt{\frac{S_1^2}{n}+\frac{S_2^2}{n}}}\sim t(m)
$$

其中的 $m$ 根据 Behrens-Fisher 问题给出的近似值代入。

# 四种分布的关联
* 正态分布是老祖宗。
  * 对于求和具有保持性，进一步有中心极限定理。
  * 对于线性组合具有保持性。
  * “求和”
* $\chi^2$ 分布的重要性仅次于正态分布。
  * 对于加和具有保持性。
  * 对于线性组合也具有保持性。
  * 从正态分布进入 $\chi^2$ 分布，关键在于平方。
  * “平方求和”
* $t$ 分布**大体**是标准正态分布与 $\chi^2$ 分布均方根的比值。
  * “样本标准化”
* $F$ 分布**大体**是两个 $\chi^2$ 分布各自平均后的比值。
  * “方差的比较”

