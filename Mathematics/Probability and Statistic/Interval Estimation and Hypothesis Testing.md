# 说明
统计里面的区间估计和假设检验有密切联系。这种关系就好比，正向推导与反证法的关系。下面给出一些参量和术语的对应关系。


|区间估计的统计量|假设检验的统计量|
|-|-|
|枢轴量<br>Pivotal Value|检验统计量<br>Test Statistics|
|置信水平<br> $1-\alpha$ |检验水平<br> $\alpha$ |
|临界值|临界值|
|置信区间|否定域|

这二者本质上解决的问题相同，描述的现象相同，但是处理问题的流程和思路又有区别，需要注意。

# 假设检验中的检验统计量
补充一下，关于等号的检验，都可以修正转化为对不等号的检验，相当于和临界值比较之前，进行了不等式放缩。

这一点要在考试前重点复习一下。

另外，两个正态总体的比较是常见的问题。**方差的比较一般做比，而期望的比较一般做差**。这么做的道理在于，正态分布的线性组合（这里是做差）还是正态分布。而 $\chi^2$ 分布对于求和可以保持，对于做差就不可以了。恰好做比，可以构造另一种已知的分布，也就是 $F$ 分布。

## 服从 $N(0,1)$ 分布的检验统计量
很多问题，只要样本量足够大，都可以近似用标准正态分布检验。

构造这类检验统计量的**出发点是已知一个满足正态分布的统计量，思路是对它标准化**。
### 一个正态总体的期望值检验（已知方差）
已知 $\sigma^2,H_0:\mu=\mu_0$

$$
N=\frac{\bar X-\mu}{\sqrt{\frac{\sigma^2}{n}}}=\frac{\bar X-\mu_0}{\sqrt{\frac{\sigma^2}{n}}}\sim N(0,1)
$$

### 两个 Bernoulli 总体的成功率（比率）检验
虽然独立同分布的 $X_i$ 各自服从两点分布(Bernoulli分布) ，它们的和 $\sum X_i$ 服从二项分布。

但是，所谓的比率不是它们中的任何一个，而是作为随机变量的均值： $\bar{X}=p$ 。

均值的标准化近似服从标准正态分布，这是中心极限定理保证的。所以，尽管是 Bernoulli 总体，检验统计量（在零假设之下）和枢轴量都是服从标准正态分布的。

$H_0: p_1\le p_2$

$$
\hat p_1 = \frac{1}{n_1}\sum_{i}^{n_1} X_i=\frac{S_1}{n_1}
$$

$$
\hat p_2 = \frac{1}{n_2}\sum_{i}^{n_2} Y_i=\frac{S_2}{n_2}
$$

$$
\xi = \frac{\hat p_1-\hat p_2-(p_1-p_2)}{\sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2}}}=\frac{\hat p_1-\hat p_2}{\sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2}}}\sim N(0,1)
$$

上面的，去做单边检验。

$H_0: p_1 = p_2$

$$
\hat p = \frac{1}{n_1+n_2}(\sum_{i}^{n_1} X_i+\sum_{j}^{n_2} Y_j)
$$



$$
\xi = \frac{\hat p_1-\hat p_2}{\sqrt{(\frac{1}{n_1}+\frac{1}{n_2})\hat{p}(1-\hat{p})}}\sim N(0,1)
$$

上面的去做双边检验。

## 服从 $\chi^2$ 分布的检验统计量
$\chi^2$ 分布的定义是若干标准正态分布的平方和。

从这样的定义可以看出，涉及平方和的假设检验往往离不开 $\chi^2$ 分布。

什么地方会涉及平方和？除了方差之外？

平方和描述的是一种二范数下定义的广义“距离”。那么，凡是涉及到“距离”或者和某一个固定参考点的“差异”比较问题，就往往会涉及到平方和。方差本身，也是描述了样本到其均值的“距离”。

具体构造检验统计量，不仅仅是求平方和。还要在平方求和以前，逐项标准化（各项服从正态分布）。

也就是说，构造这一类统计量的**出发点是“距离”和某一定值的比较**，构造**思路是逐项标准化，然后求平方和**。
### 分布函数检验（拟合优度检验）
已知分布是零假设，
可以有力地否定分布函数。


对 $\nu_i$ 进行标准化然后求平方和。

$$
V=\sum_{i=1}^{m+1}\frac{(\nu_i-np_i)^2}{np_i}\sim \chi^2(m)
$$

注意这是一个单边检验，因为原假设对应的是 $V\to 0$。 

### 独立性检验
独立性是零假设，
可以有力地否定独立性。

$$
\sum_{i,j\in \{1,2\},i\neq j}\frac{(nP(A_iA_j)-nP(A_i)P(A_j))^2}{(nP(A_i)P(A_j))^2}\sim \chi^2(1)
$$

注意这是一个单边检验，因为原假设对应的是 $V\to 0$。

### 一个正态总体的方差检验

$H_0:\sigma^2=\sigma _0^2$

$$
W=\sum_{i=1}^n \frac{(X_i-\bar X)}{\sigma^2}
=\sum_{i=1}^n \frac{(X_i-\bar X)}{\sigma_0^2}\sim \chi^2(n-1)
$$

注意这是一个双边检验。不等号才对应单边检验。


## 服从 $F$ 分布的检验统计量
$F$ 分布的定义和方差做比有密切关系。所以这一类检验统计量的构造的思路是，做比。并且，构造的前提，具有平方和的属性。

$F$ 分布的定义是，两个 $\chi^2$ 分布各自平均（除以自由度）之后的比值。

所以， $F$ 分布类型的检验统计量的构造出**发点是比较两个总体在某种意义下的“距离”，思路是首先各自构造 $\chi^2$ 类型统计量，然后各自平均（除以自由度），最后做比**。
### 线性回归问题中的相关性检验
线性无关是零假设，
可以有力地肯定线性相关性。

$$
F=\frac{U}{\frac{Q}{n-2}}\sim F(1,n-2)
$$

这里的 $U,Q$ 分别为回归平方和与残差平方和。

$$
U=\sum_t(\bar y-\hat y_t)^2=\hat b^2 l_{XX}
$$

$$
Q=\sum_t(y_t-\hat y_t)^2=l_{YY}-U
$$

这种检验等价于用相关系数 $R$ 进行检验。

注意这是一个单边检验，因为零假设对应 $F\to 0$

### 两个正态总体的方差检验
####  $H_0:\sigma_1^2=\sigma_2^2$

$$
F=\frac{\frac{S_1^2}{\sigma_1^2}}{\frac{S_2^2}{\sigma_2^2}}=\frac{S_1^2}{S_2^2}\sim F(n_1-1,n_2-1)
$$

### 一个 Bernoulli 总体成功率（比率）的假设检验
具体比较复杂，但是临界值的计算涉及到 $F$ 分布。


## 服从 $t$ 分布的检验统计量
这种做法和正态分布的检验统计量构造思路一致，都是标准化（减去期望值，除以标准差）。只不过，标准差是通过样本估计出来的，而不是明确已知的。所以，主要用于对期望值的假设检验（利用样本均值）。

另外， $t$ 分布的定义本身就和标准化的过程密切相关，是一个标准正态分布，除以一个 $\chi^2$ 分布的均方根。自由度是继承的关系。

总结一下， $t$ 分布类型检验统计量构造的**出发点是某一个服从正态分布的统计量，并且对其的标准化遇到困难，因为方差未知。思路就是标准化，并且用样本方差估计未知的总体方差**。

另外， $t$ 分布的平方满足特殊的 $F$ 分布。
### 一个正态总体的期望值检验
未知 $\sigma^2,H_0:\mu=\mu_0$

$$
T=\frac{\bar X-\mu}{\sqrt{\frac{S^2}{n}}}=\frac{\bar X-\mu_0}{\sqrt{\frac{S^2}{n}}}\sim T(n-1)
$$

### 成对数据的期望值检验
$H_0:\mu_z=0$
做差化成正态单总体（差值服从正态分布，各自未必服从）

$$
Z=X-Y
$$

转化为正态单总体的情况。

### 两个正态总体的期望值检验


#### 已知 $\sigma_1^2=\sigma_2^2,\ H_0:\mu_1=\mu_2$ 
对 $\bar X-\bar Y$ 标准化：


$$
T=\frac{\bar X-\bar Y-(\mu_1-\mu_2)}{\sqrt{\frac{S_1^2}{n}+\frac{S_2^2}{n}}}=\frac{\bar X-\bar Y}{\sqrt{\frac{S_1^2}{n}+\frac{S_2^2}{n}}}\sim t(2n-2)
$$

当样本量不一样的时候，也对 $\bar X-\bar Y$ 标准化：
区别在于（其实是更一般的情形），要用方差的最小方差无偏估计量代入：

$$
\hat{\sigma^2}=\frac{\sum_i(X_i-\bar{X})^2+\sum_j(Y_j-\bar{Y})^2}{n_1+n_2-2}
$$


$$
T=\frac{\bar X-\bar Y-(\mu_1-\mu_2)}{\sqrt{\frac{\hat{\sigma^2}}{n_1}+\frac{\hat{\sigma^2}}{n_2}}}=\frac{\bar X-\bar Y}{\sqrt{\frac{\hat{\sigma^2}}{n}+\frac{\hat{\sigma^2}}{n}}}\sim t(n_1+n_2-2)
$$

#### 已知 $\sigma_1^2\neq\sigma_2^2,\ H_0:\mu_1=\mu_2$ 

对 $\bar X-\bar Y$ 标准化：


$$
T=\frac{\bar X-\bar Y-(\mu_1-\mu_2)}{\sqrt{\frac{S_1^2}{n}+\frac{S_2^2}{n}}}=\frac{\bar X-\bar Y}{\sqrt{\frac{S_1^2}{n}+\frac{S_2^2}{n}}}\sim t(m)
$$

其中的 $m$ 根据 Behrens-Fisher 问题给出的近似值代入。

# 四种分布的关联
* 正态分布是老祖宗。
  * 对于求和具有保持性，进一步有中心极限定理。
  * 对于线性组合具有保持性。
  * “求和”
* $\chi^2$ 分布的重要性仅次于正态分布。
  * 对于加和具有保持性。
  * 对于线性组合也具有保持性。
  * 从正态分布进入 $\chi^2$ 分布，关键在于平方。
  * “平方求和”
* $t$ 分布**大体**是标准正态分布与 $\chi^2$ 分布均方根的比值。
  * “样本标准化”
* $F$ 分布**大体**是两个 $\chi^2$ 分布各自平均后的比值。
  * “方差的比较”
# 假设检验的根本逻辑
上面讨论的几种假设检验问题是很有局限性的。一方面随机变量的类型局限于正态总体，另一方面检验的命题局限于期望、方差、比率、分布、独立性等指定的命题。

然而假设检验的思路是普遍性的，对于随机变量服从的分布没有限制，对于检验的命题也没有限制。那么提取共性，假设检验的核心思路是下面这样子的：

## 希望冒着犯第一类错误的风险做出决策
1. 提出希望证实的命题作为备择假设 (Alternative Hypothesis)
2. 把它的否定作为零假设 (Null Hypothesis)
3. 在零假设的条件下，计算已然得到的观测现象，发生的概率
   1. 这个概率一般而言，计算的不是等号，而是不等号，涉及放缩
   2. 为了假设检验，否定原假设（概率意义下的反证法）能够实现，应该：
      1. 期待计算出来的这个概率很小
      2. 给出这个小概率的上界（也应该很小）
      3. 也就是要计算观测现象，发生概率的上界
      4. 等价于计算观测现象的对立事件，发生概率的下届
4. 冒着犯第一类错误的风险，否定原假设，肯定备择假设
   1. 不是所有情况下，都可以否定原假设的。需要计算出来的概率小于检验水平。
      1. 根据样本数据的具体取值，计算出来的可以否定原假设的，检验水平的，最小值，就是p值
         1. 另外一个角度讲，根据样本数据，计算出来检验统计量的一个具体取值。在零假设条件下，检验统计量作为一个随机变量，取值比这个具体取值更夸张（偏离期望值的程度更大）的概率，就是p值
      2. 如果检验水平大于p值，则可以否定原假设
      3. 如果检验水平小于p值，就不能否定原假设
   2. 第一类错误就是，不该否定而否定了。也就是以真为假
   3. 这一类错误的概率，不超过检验水平
   4. 是可以被有效控制的
## 希望冒着犯第二类错误的风险做出决策
1. 提出希望证实的命题
2. 把它作为零假设 (Null Hypothesis)
3. 在零假设的条件下，计算已然得到的观测现象，发生的概率
   1. 这个概率一般而言，计算的不是等号，而是不等号，涉及放缩
   2. 假设检验逻辑下，应该：
      1. 期待计算出来的这个概率不太大
      2. 给出这个小概率的上界（也应该不太大）
      3. 也就是要计算观测现象，发生概率的上界
      4. 等价于计算观测现象的对立事件，发生概率的下届
4. 冒着犯第二类错误的风险，不否定原假设，认为二者相容
   1. 不是所有情况下，都可以二者相容的。需要计算出来的概率大于检验水平。
      1. 根据样本数据的具体取值，计算出来的可以否定原假设的，检验水平的，最小值，就是p值
         1. 换句话说，也是，根据样本数据的具体取值，计算出来的可以认为和原假设相容的，检验水平的，最大值，就是p值
         2. 另外一个角度讲，根据样本数据，计算出来检验统计量的一个具体取值。在零假设条件下，检验统计量作为一个随机变量，取值比这个具体取值更夸张（偏离期望值的程度更大）的概率，就是p值
      2. 如果检验水平大于p值，则可以否定原假设
      3. 如果检验水平小于p值，就不能否定原假设
   2. 第二类错误就是，该否定而未否定。也就是以假为真
   3. 这二类错误的概率，是不能有效控制的
   4. 但是可以通过增大样本量，减小之
## 没有事先的期待，随缘的检验（仅限于临界值法）
1. 提出希望判别的命题，作为零假设
2. 在零假设条件下，计算观察到的现象的概率的上界，也就是其对立事件概率的下界。
3. 根据给定的检验水平
   1. 如果概率的上界小于检验水平就否定零假设
   2. 如果概率的上界大于检验水平就认为实验现象和零假设相容
# 概念辨析
## 临界值 检验水平 与 p值 的关系
* 从“量纲”的角度来看，临界值的量纲是和检验统计量一样的。检验水平和p值的量纲，都是1，都是某种概率
* 临界值法的逻辑是这样的
  * 给定检验水平
  * 根据检验水平，查表计算临界值
  * 根据临界值，在检验统计量的分布范围上，划分否定域
  * 否定域是偏离期望值的极端的区域
  * 临界值会使得否定域对应的概率小于等于检验水平
    * 这样导致下面的结果
    * 当实验现象对应的事件落在否定域内部，这个事件发生的概率不超过否定域的概率
    * 而否定域的概率不超过检验水平
    * 所以检验水平有效的控制（不等号严格放缩）了零假设条件下，观测现象发生的概率
    * 检验水平就是零假设条件下，观测事件发生概率的上界（当然不是上确界）
* p值方法的逻辑是这样的
  * 根据实验数据，计算检验统计量的具体取值
  * 检验统计量作为随机变量，它的概率分布已知
  * 检验统计量取值，比这个具体取值更极端的概率之和就是p值
    * p值对应了检验统计量在比具体取值更极端的范围内，的概率之和
  * 如果给出的检验水平大于p值，就可以否定零假设
    * 从这个意义上说，p值是可以否定零假设的检验水平的取值的下确界
* 从否定域的划分来看
  * 临界值法，划分在检验统计量（随机变量）的取值范围上
  * p值方法，划分在概率取值范围上
    * 这个概率是检验统计量比具体取值更极端（偏离期望值）的概率之和
* 从给定条件来看
  * 临界值法，从检验水平出发。要事先给定检验水平，通过临界值，确定相应的否定域，然后判断实验结果对应的检验统计量的具体取值，是不是落在了否定域内部
  * p值方法，从实验数据出发。不需要事先给定检验水平，而是默认实验现象落在否定域内部，给出了对应检验水平的下确界。或者说，它把否定域直接划分在了极端现象（比实验现象更偏离零假设下的期望现象）之和的概率上，直接判别检验水平和p值的关系就可以了
* 从结论上看
  * 临界值法给定的结论，有可能否定零假设，也可能认为相容。这取决于具体的实验数据。
  * p值方法给出的结论是，根据实验数据，要想否定原假设，检验水平最小是多少，也就是因为“否定原假设而犯错误（第一类错误）”的概率的上界（这里上界对应的是“和”，比实验现象更极端现象的“和”的概率）的下确界。
## 看待p值的几种观点
* 比观测到的实验现象，更极端事件的和，的概率
  * 更极端指的是更加偏离期望值
  * 一般是通过标准化成为检验统计量，和指的是检验统计量（随机变量表示随机事件）对应事件之和，期望也就检验统计量的期望值而言
* 为了否定原假设，检验水平取值的下确界
* 确实否定了原假设，犯错误概率的上界的下确界
  * 首先说上界，是因为犯错误概率的上界就是更极端事件的和的概率（这当然是一个确定的上界，但也不一定是上确界）
  * 因为这个上界是确定的
  * 而一般的上界是可以无限大的
  * 所有这是一个上界的下确界
## 检验统计量与枢轴量的区别
* 在我上面的公式整理中，左侧的式子是枢轴量，右侧的式子是检验统计量。
* 枢轴量将零假设代入，就变成了检验统计量
* 枢轴量和真实参数相关，它的分布和真实参数无关
* 检验统计量和真实参数无关（和零假设的形式参数相关），它的分布和真实参数相关
* 枢轴量永远满足已知的标准分布
* 检验统计量只有在（等号的）零假设条件下，才满足已知的标准分布
## 单边检验和双边检验的区别
* 单边检验对应不等号的假设检验（配合不等式放缩）
* 双边检验对应等号的假设检验
* 等号与不等号，单边与双边
  * 使用相同的检验统计量
  * 但是使用不同的临界值和否定域